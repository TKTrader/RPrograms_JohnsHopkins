---
title: "Practical Machine Learning Course Project"
author: "Thomson Kneeland"
date: "June 14, 2016"
output: pdf_document
---

```{r global_options, include = FALSE}
knitr::opts_chunk$set(fig.height=4,echo=TRUE, warning=FALSE, message=FALSE)
```

## Executive Summary
Using the Weight Lifting Exercises Dataset, we examined the execution of the Unilateral Dumbbell
Biceps Curl with 6 participants performing 10 repetitions in 5 different fashions: exactly according
to the specification (Class A), throwing the elbows to the front(Class B), lifting the dumbbell only
halfway(Class C), lowering the dumbbell only halfway(Class D) and throwing the hips to the front
(Class E). 
(See http://groupware.les.inf.puc-rio.br/har for more details.)

Our goal is to create a viable machine learning algorithm to identify the class of future observations
based on the data presented.  The first model attempted was a decision tree model which yielded only 49%
accuracy and was dismissed. A second model used gradient boosting and yielded a 96.3% accuracy (out of 
sample error of 3.6%), faring quite well for prediction purposes.  However, a third model using random 
forests fared even better with an accuracy of 99.3% and an out-of-bag (OOB) estimate of error of .43%. 

## Data Preparation and Cleaning
First we load our training and test data along with any packages needed for data processing.  We also
account for missing variable entries by providing NA values.
```{r, echo=TRUE}
data.train <- read.csv("pml-training.csv", stringsAsFactors = FALSE, header=TRUE,
                       na.strings=c("","#DIV/0!"))
data.test <- read.csv("pml-testing.csv", stringsAsFactors = FALSE, header=TRUE,
                      na.strings=c("","#DIV/0!"))
library(caret)
library(dplyr)
library(rpart)
library(rattle)
library(randomForest)
```
The first seven variable columns do not feature data that we need for processing.
```{r, echo=TRUE}
data.train <- select(data.train, -(1:7))
data.test <- select(data.test, -(1:7))
```
Variables with nearly zero variance will not help with our predictions, so we remove those. 
```{r, echo=TRUE}
nzv <- nearZeroVar(data.train)
data.train <- data.train[, -nzv]
```
Some columns feature all NA values, so those will be removed as well.
```{r, echo=TRUE}
var.comp <- names(data.train[,colSums(is.na(data.train)) == 0])
data.train <- data.train[, var.comp]
```
The final dataset now consists of 52 variables and our outcome (classe).
Finally, we will also subset the training data for cross validation purposes in our decision tree model
with a 70/30% allocation.
```{r, echo=TRUE}
set.seed(412)
sub <- createDataPartition(data.train$classe, p = .7,list=FALSE) 
data.train.1 <- data.train[sub,]
data.train.2 <- data.train[-sub,]
```
## Decision Tree Model
We build a decision tree model on the first partition (70%) of our training data.
```{r, echo=TRUE}
model.tree <- train(classe ~ .,method="rpart",data=data.train.1)
fancyRpartPlot(model.tree$finalModel,sub="")
```

Using this model, we will predict and verify the outcomes of the second partition (30%) of our training data
for cross validation.
```{r, echo=TRUE}
tree.predict <- predict(model.tree,newdata=data.train.2)
confusionMatrix(tree.predict, data.train.2$classe)
```
The decision tree model on the partitioned data yields a prediction accuracy of 49.1%, not a good fit;
the Kappa statistic of .33 is unimpressive as a measure of matching the outcome. We should seek another
model with greater accuracy.

## Gradient Boosting Model
We next fit a gradient boosting model on the partitioned data. A preliminary run gives us best fit 
parameters of 150 trees and more, so we have added these to our model to save extensive calculation time.
```{r, echo=TRUE}
model.gbm <- train(classe ~ .,method="gbm",data=data.train.1,verbose=FALSE)
gbm.predict2 <- predict(model.gbm,newdata=data.train.2,n.trees=150,interaction.depth = 3,shrinkage = 0.1,
                        n.minobsinnode = 10)
gbm.predict <- predict(model.gbm,newdata=data.train.2)
confusionMatrix(gbm.predict, data.train.2$classe)
```
The gbm model fares much better, with a 96.3% accuracy rate in predicting our out of sample data with
a Kappa statistic of .954.  This does seem like a great model fit, but we will explore a random forest
model to see if it will outperform.

## Random Forest Model
The random forest function cross validates internally, so we will use a 3-fold cross validation on
the full training data.  The model will grow 200 trees, enough trees for accuracy and insuring
input rows are predicted multiple times, while lowering processing time.
```{r, echo=TRUE}
model.rf <- train(classe ~ .,method="rf",ntree=200,data=data.train,
                  trControl=trainControl(method="cv",number=3),keep.forest=TRUE)
model.rf
```
All three models produce stellar results > 98.5% accuracy with very high Kappa statistic values > .98.
```{r, echo=TRUE}
model.rf$finalModel
```
This best fit model features an accuracy of 99.3% using 27 variables, far outperforming the  decision 
tree model and being a perfect candidate for future prediction.  The out-of-bag (OOB) error estimate 
for this model is .43%, supporting its strength with a low predictive error. Accordingly, the classification
error is minimal and less than 1% for all outcomes.  The OOB error estimate removes the need for an out of
sample error rate, since 1/3 of the training data set was used as "out of bag samples". We can expect this
error rate to remain similar in predicting future outcomes.

## Conclusions and Test Set Predictions
The Decision Tree model performed inadequately, only predicting 49.1% of outcomes. The GBM and random forest
models performed quite well with an accuracy of 96.3% and 99.3% respectively. The random forest model was
selected as the best fit and we will use it on our test data. Accordingly, we predict the
following results for classe with the random forest model:
```{r, echo=TRUE}
predict(model.rf,data.test)
```
